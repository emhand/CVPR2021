Emily specifically asked if the citations 16, 1, 6, and 23 from the WebCaricature paper were worth using/comparing

16:  Brendan F Klare, Serhat S Bucak, Anil K Jain, and Tayfun Akgul. Towards automated caricature recognition. InProc. Int. Conf. Biometrics, pages 139–146, 2012 (https://ieeexplore.ieee.org/document/6199771). This caricature dataset consists of a photo and caricature pair across 196 subjects. 89 pairs were collected from caricature artists, and 107 pairs were collected from Google Image searches. The authors tried to select face photographs withminimal variation in pose, illumination, and expression. The authors define "levels" of qualitative features, where leve1 pertains to general feturs, and level 2 pertains to specific features (such as "sleepy eyes, mouth width, etc"). Every image was then labeled using mechanical turk using the 25 prescribed features (3 ind labelers). The entire pipelines takes the labeled qualitative features, calculates a histogram represenation, takes the histogram represenation to get the difference vector, and then feeds the difference vector to mutiple kernel learning, lagistic regression, or SVM to get a similarity score. Also, 2/3 to 1/3 for splits using 10 fold cross val. The paper says the dataset is publically available, but I cannot find it (maybe because it's so old?)

1: Bahri Abaci and Tayfun Akgul.  Matching caricatures to photographs. Signal, Image and Video Processing, 9(1):295–303, 2015. Paper here: https://link.springer.com/article/10.1007/s11760-015-0819-8 Dataset here: https://www.gag.itu.edu.tr/CPdatabase/ (Need to request password) This caricature dataset consists of 200 commissioned gray level, hand drawn cariactures and their corresponding photomates. The authors argue that since caricatures are unrealistic and subjective, appearance-based (skin color, facial depth) and geometry-based features (facial ratios) can't be used for match/rec. Instead, they argue that features that aren't affected by exaggeration should be found (m/f, nose size, etc), then the qualitative features labeled by independent labelers. Pattern recognition is used to extract 23/32 features. First, facial landmarking is used on the photos. Then textural analysis is applied, the eyes are aligned and geometric feature extractirion is applied. Then textural features are extracted (LBP, edge gradient magnitudes, edge gradient orientation). Each extracted feature is then classified by a classifier (dependent on the feature). The goal here is to determine if that feature is present, not if the images match. After training the classifier, the extracted features are then used to match caricatures with photos by minimizing a distance metric (euc). A GA mearures the importance of each attribute (shocker, gender and glasses are top).

6:   Elliot J. Crowley, Omkar M. Parkhi, and Andrew Zisserman.  Face painting: queryingart with photos. In Proc. Brit. Mach. Vis. Conf., pages 65.1–65.13, September 2015. https://www.robots.ox.ac.uk/~vgg/publications/2015/Crowley15/ This dataset is painting, not caricatures? Not even sure why they bother to mention it. Not going to waste my time on this.

23:Nandan Rai S. Mishra A. Mishra, A. and C. V. Jawahar. Iiit-cfw: A benchmark database of cartoon faces in the wild. InVASE ECCVW, pages 35–47, 2016 Paper: http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2016/Mishra-ECCVW2016.pdf DataSet: http://cvit.iiit.ac.in/research/projects/cvit-projects/cartoonfaces This one actually does contain caricatures, but not all caricatures. TBH, probably about the same quality as WebCaricature. The entire paper just introduces the dataset and interesting cartoon problems. 
